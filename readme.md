# ðŸ† Neural Arena â€” Real-Time LLM Combat Bot (Strategy + Reflex, Zero Lag)

> A retro top-down arena shooter where you fight an AI opponent powered by an LLM â€” but engineered like a **real game agent**:  
> **LLM = strategy**, **Reflex Layer = frame-perfect execution**, **Prediction = latency compensation**, **Queue = stability**, **Memory = adaptation**.

---

## ðŸ–¼ï¸ Architecture Diagram

![Neural Arena Workflow](workflow.png)

---

## ðŸ” How the System Works (End-to-End)

Neural Arena is built around one principle:

**Gameplay must never wait on inference.**  
So we split the AI into **three loops** that run at different speeds but stay synchronized with strict contracts.

### 1) Loop A â€” Real-Time Simulation (60 FPS)
This is the core game loop. It runs continuously and never blocks:
- Updates player + bot movement
- Simulates bullets, collisions, cooldowns, damage
- Renders the arena
- Emits an immutable **State Snapshot** every tick

**State Snapshot (example contents):**
- Player pose + velocity
- Bot pose + velocity
- Health/cooldowns
- Top-K nearby bullets/threats
- Obstacle/wall proximity signals

âœ… Result: the game always feels smooth and responsive.

---

### 2) Loop B â€” LLM Planning Pipeline (Async, pipelined ~250ms)
LLMs are strong at **strategy**, but too slow for micro-reactions.  
So the LLM never directly drives frame-by-frame controls. It outputs a **compact intent**.

#### Step B1 â€” Prediction Model (Latency Compensation)
To avoid the bot â€œthinking behind reality,â€ we predict where the player will be when the command lands.

Dead-reckoning idea:
`p_future = p_now + v_now * Î”t`

Where `Î”t` approximates network + model latency.

We produce a **Future Snapshot** that reflects the likely state at execution time.

#### Step B2 â€” Async + Pipelined LLM Calls
We keep multiple LLM calls in-flight:
- While one request is processing, the next snapshot is already queued
- The bot always has a fresh decision incoming
- This eliminates the â€œpauseâ€¦ thinkâ€¦ moveâ€ effect

#### Step B3 â€” Strict Output: Intent Packet
The LLM returns an **Intent Packet** â€” not raw actions:
- `dx, dy` movement bias (âˆ’1..1)
- `shoot` / `shoot_probability`
- optional `goal` / `aggression`
- `tick_id` (ordering + staleness protection)

#### Step B4 â€” Staleness Protection (Critical)
Because async calls can return out of order:
- Every request includes `tick_id`
- Every response must echo it
- Older responses are discarded automatically

âœ… Result: the bot stays strategic without becoming â€œlate.â€

---

### 3) Loop C â€” Reflex Execution (10â€“20ms)
This layer runs locally and fast. It is responsible for **real-time feel**.

It reads the latest valid intent from the **Intent Queue** and:
- Converts intent into frame-by-frame controls
- Smooths movement so the bot feels continuous
- Enforces **actuation guardrails** (no wall clipping, obey cooldowns)
- Performs emergency dodging when threats are imminent

âœ… Result: the bot reacts like a real player even if the LLM takes hundreds of milliseconds.

---

## ðŸ§  Long-Term Memory (Adaptive Opponent)
We donâ€™t store unlimited text. We store **bounded, measurable features** that actually improve play:
- Player style classification (strafer / rusher / camper)
- Typical dodge direction
- Preferred engagement distance
- Pressure response (hit rate under threat)

These features are injected into future prompts so the AI **adapts across rounds**, not just within one.

---

## ðŸš€ Why This Wins (What Judges Actually Care About)

### âœ… Not a demo gimmick â€” a real architecture
Most â€œLLM botsâ€ are:
- slow
- jittery
- random
- prompt-dependent

Neural Arena is:
- **engineered for latency**
- **stable under async**
- **measurable**
- **game-feel first**

### âœ… The key insight
> LLMs cannot do frame-perfect reactions â€” so we *donâ€™t ask them to*.  
> We let the LLM plan and let a Reflex Layer execute.

---

## ðŸŽ® Gameplay

- **You (human)**: move + shoot in real time  
- **AI opponent**:
  - Plans strategically via LLM
  - Executes tactically via Reflex Layer
  - Improves via Memory

Arena includes:
- Boundaries + obstacles
- Projectile combat
- Cooldowns / health system
- Bullet ownership rules (no self-damage)

---

## ðŸ§© Modules

### Front End
- **Human Input** â†’ controls the player
- **Game Display** â†’ renders at 60 FPS
- **Movement Translator** â†’ converts AI intent into game control signals

### Back End
- **Game Engine** â†’ physics + collisions + state snapshots
- **Prediction Model** â†’ latency compensation for future-state planning
- **Async LLM API** â†’ pipelined strategic decisions
- **Queue** â†’ latest-wins intent stabilization
- **Reflex Layer** â†’ fast execution + safety guardrails
- **Long-Term Memory** â†’ adaptation features across rounds

---

## ðŸ“¦ Data Contracts (LLM I/O)

### Input (State/Future Snapshot)
We keep this small and structured:
- Player + bot pose/velocity
- Threat summary (top-K bullets: position + velocity)
- Cooldowns + health
- Obstacles/walls proximity
- Memory feature vector

### Output (Intent Packet)
Compact JSON intent:
- `dx, dy` (movement bias)
- `shoot` or `shoot_probability`
- optional `aggression` or `goal`
- `tick_id`

---

## ðŸ“Š Observability (Hackathon-Grade Proof)
We track metrics because â€œit feels betterâ€ isnâ€™t enough:
- average LLM latency (ms)
- in-flight request count
- stale discard rate (%)
- intent queue age (ms)
- bot hit rate / survival time
- dodge success rate

---

## ðŸ› ï¸ Local Setup

> Replace these commands with your repoâ€™s real ones (Python or Web).  
> The structure below is designed to be easy for judges to run.

### 1) Clone
```bash
git clone https://github.com/<your-org>/<your-repo>.git
cd <your-repo>